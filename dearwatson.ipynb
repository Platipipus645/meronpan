{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\n\n# Check available input files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Read the training data\ntrain = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntrain.head()\n\n# Print premise, hypothesis, and relationship\nprint('Premise:')\nprint(train['premise'].iloc[1])\nprint('\\nHypothesis:')\nprint(train['hypothesis'].iloc[1])\nprint('\\nRelationship:')\nprint(train['label'].iloc[1])\n\n# Find the longest length\nlongest_len = 0\nidx = 0\nfor index, row in train.iterrows():\n    premise_len = len(row['premise'].split())\n    hypothesis_len = len(row['hypothesis'].split())\n    length = 3 + premise_len + hypothesis_len\n    if length > longest_len:\n        longest_len = length\n        idx = index\n\nmax_len = longest_len + 100\n\nprint('Longest Length:', longest_len)\nprint('Index:', idx)\nprint('Premise:')\nprint(train['premise'].iloc[idx])\nprint(f\"Length: {len(train['premise'].iloc[idx].split())}\")\nprint('Hypothesis:')\nprint(train['hypothesis'].iloc[idx])\nprint(f\"Length: {len(train['hypothesis'].iloc[idx].split())}\")\n\ndef bert_encode(hypotheses, premises):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    num_examples = len(hypotheses)\n    print(f'Encoding {num_examples} pairs of hypotheses and premises as inputs...')\n\n    sentence1 = [\n        tokenizer.encode(s, add_special_tokens=True)\n        for s in np.array(hypotheses)\n    ]\n    sentence2 = [\n        tokenizer.encode(s, add_special_tokens=True)\n        for s in np.array(premises)\n    ]\n\n    sentence1 = tf.keras.preprocessing.sequence.pad_sequences(sentence1, padding='post')\n    sentence2 = tf.keras.preprocessing.sequence.pad_sequences(sentence2, padding='post')\n\n    print(sentence1[0])\n\n    cls = [tokenizer.cls_token_id] * sentence1.shape[0]\n    input_word_ids = tf.concat([tf.expand_dims(cls, axis=-1), sentence1, sentence2], axis=-1)\n    print(input_word_ids[0])\n\n    input_mask = tf.ones_like(input_word_ids)\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([tf.expand_dims(type_cls, axis=-1), type_s1, type_s2], axis=-1)\n\n    inputs = {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids\n    }\n\n    print('Finished')\n\n    return inputs\n\ntrain_input = bert_encode(train['hypothesis'].values, train['premise'].values)\nprint(train_input['input_word_ids'][0])\nprint(train_input['input_word_ids'][idx])\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\n\n    input_word_ids = tf.keras.Input(\n        shape=(None,),\n        dtype=tf.int32,\n        name=\"input_word_ids\")\n    input_mask = tf.keras.Input(\n        shape=(None,),\n        dtype=tf.int32,\n        name=\"input_mask\")\n    input_type_ids = tf.keras.Input(\n        shape=(None,),\n        dtype=tf.int32,\n        name=\"input_type_ids\")\n\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\nmodel = build_model()\nmodel.summary()\n\nmodel.fit(train_input, train['label'].values, epochs=5, verbose=1, batch_size=16)\n\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_input = bert_encode(test['hypothesis'].values, test['premise'].values)\ntest.head()\n\npredictions = [np.argmax(i) for i in model.predict(test_input)]\nsubmission = test['id'].copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-02T12:32:37.400331Z","iopub.execute_input":"2023-06-02T12:32:37.400767Z","iopub.status.idle":"2023-06-02T13:29:07.957657Z","shell.execute_reply.started":"2023-06-02T12:32:37.400733Z","shell.execute_reply":"2023-06-02T13:29:07.956585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:31:16.454084Z","iopub.execute_input":"2023-06-02T13:31:16.454444Z","iopub.status.idle":"2023-06-02T13:31:16.474511Z","shell.execute_reply.started":"2023-06-02T13:31:16.454414Z","shell.execute_reply":"2023-06-02T13:31:16.473543Z"},"trusted":true},"execution_count":null,"outputs":[]}]}