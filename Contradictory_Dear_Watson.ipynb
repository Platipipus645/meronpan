{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-01T12:44:47.832588Z","iopub.execute_input":"2023-06-01T12:44:47.833114Z","iopub.status.idle":"2023-06-01T12:44:47.886496Z","shell.execute_reply.started":"2023-06-01T12:44:47.833078Z","shell.execute_reply":"2023-06-01T12:44:47.885351Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/contradictory-my-dear-watson/sample_submission.csv\n/kaggle/input/contradictory-my-dear-watson/train.csv\n/kaggle/input/contradictory-my-dear-watson/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"premise = train.loc[1, 'premise']\nhypothesis = train.loc[1, 'hypothesis']\nrelationship = train.loc[1, 'label']\n\nprint('Premise:')\nprint(premise)\nprint('\\nHypothesis:')\nprint(hypothesis)\nprint('\\nRelationship:')\nprint(relationship)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"longest_len = 0\nidx = 0\nfor index, row in train.iterrows():\n    premise_len = len(row['premise'].split())\n    hypothesis_len = len(row['hypothesis'].split())\n    length = 3 + premise_len + hypothesis_len\n    if length > longest_len:\n        longest_len = length\n        idx = index\n\nmax_len = longest_len + 100\n\nprint('Longest Length:', longest_len)\nprint('Index:', idx)\nprint('Premise:')\nprint(train.loc[idx, 'premise'])\nprint(f\"Length: {len(train.loc[idx, 'premise'].split())}\")\nprint('Hypothesis:')\nprint(train.loc[idx, 'hypothesis'])\nprint(f\"Length: {len(train.loc[idx, 'hypothesis'].split())}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef demo_encode():\n    example = 'Hello world!'\n    tokens = list(tokenizer.tokenize(example))\n    tokens.append('[SEP]')\n    print('Demo using:\\n\\\"%s\\\"\\n' % example)\n    print('Tokens:\\n', tokens)\n    print('IDs:\\n', tokenizer.convert_tokens_to_ids(tokens))\n\ndemo_encode()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(encode_sentence(\"Hello World!\"))\n\nprint(\"Encode longest:\")\nlong_encode = encode_sentence(train.premise.values[idx])\nprint(encode_sentence(train.premise.values[idx]))\nprint(f\"Length: {len(long_encode)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    num_examples = len(hypotheses)\n    print(f'Encoding {num_examples} pairs of hypotheses and premises as inputs...')\n    \n    sentence1 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(hypotheses)    \n    ])\n    sentence2 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(premises)\n    ])\n    \n    print(sentence1[0])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n    print(input_word_ids[0])\n    \n    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=[num_examples,max_len])\n    \n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor(shape=[num_examples,max_len])\n    \n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(shape=[num_examples,max_len]),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids\n    }\n    \n    print('Finished')\n    \n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_input['input_word_ids'][0])\nprint(tokenizer.convert_ids_to_tokens(train_input['input_word_ids'][0]))\nprint(train_input['input_word_ids'][idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    \n    input_word_ids = tf.keras.Input(\n        shape=(None,),\n        dtype=tf.int32,\n        name=\"input_word_ids\")\n    input_mask = tf.keras.Input(\n        shape=(None,),\n        dtype=tf.int32,\n        name=\"input_mask\")\n    input_type_ids = tf.keras.Input(\n        shape=(None,),\n        dtype=tf.int32,\n        name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()\n    \ntf.keras.utils.plot_model(model, \"three-input-bert-model.png\", show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_input, train.label.values, epochs=1, verbose=1, batch_size=64, validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\n\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}